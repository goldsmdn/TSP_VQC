{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c0802a-cb35-44c0-84f4-54fcf4ee9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from modules.helper_functions_tsp import ( \n",
    "    read_file_name, validate_distance_array, find_problem_size, cost_fn_fact, \n",
    "    read_index, hot_start, hot_start_list_to_string,\n",
    "    update_parameters_using_gradient, define_parameters, create_initial_rotations,\n",
    "    bind_weights, vqc_circuit, cost_func_evaluate, find_run_stats)\n",
    "from classes.DataLogger import DataLogger, SubDataLogger\n",
    "\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from modules.config import(CONTROL_FILE, ENCODING,\n",
    "                           DATA_SOURCES, CACHE_MAX_SIZE\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cadc20",
   "metadata": {},
   "source": [
    "Load control data and instantiate data logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74ef82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'locations': '4', 'slice': '1', 'shots': '1024', 'mode': '2', 'iterations': '50', 'gray': 'True', 'hot_start': 'True', 'gradient_type': 'SPSA', 'formulation': 'new', 'alpha': '0.602', 'big_a': '50', 'c': '0.314', 'eta': '0.02', 'gamma': '0.101', 's': '0.5', 'print_frequency': '50'}, 1: {'locations': '4', 'slice': '1', 'shots': '1024', 'mode': '2', 'iterations': '50', 'gray': 'False', 'hot_start': 'False', 'gradient_type': 'SPSA', 'formulation': 'new', 'alpha': '0.602', 'big_a': '50', 'c': '0.314', 'eta': '0.02', 'gamma': '0.101', 's': '0.5', 'print_frequency': '50'}}\n",
      "Data logger instantiated.  Run ID: 20250216-16-00-16\n"
     ]
    }
   ],
   "source": [
    "control_dict = read_index(CONTROL_FILE, ENCODING)\n",
    "print(control_dict)\n",
    "datalogger = DataLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bc765",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29fbde3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data logger instantiated.  Run ID: 20250216-16-00-16\n",
      "SubDataLogger instantiated.  Run ID = 20250216-16-00-16 - 16-00-16\n",
      "Folder data_sub_path = data\\20250216-16-00-16 is used for data writing\n",
      "True\n",
      "Data for Run ID: 20250216-16-00-16 - 16-00-16 successfully added to data\\20250216-16-00-16\\20250216-16-00-16.csv\n",
      "Data logger instantiated.  Run ID: 20250216-16-00-18\n",
      "SubDataLogger instantiated.  Run ID = 20250216-16-00-16 - 16-00-18\n",
      "Folder data_sub_path = data\\20250216-16-00-16 is used for data writing\n",
      "False\n",
      "Data for Run ID: 20250216-16-00-16 - 16-00-18 successfully added to data\\20250216-16-00-16\\20250216-16-00-16.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for keys, control_items in control_dict.items():\n",
    "    subdatalogger = SubDataLogger(datalogger)\n",
    "    data_dict = dict(control_items)\n",
    "\n",
    "    locations = int(data_dict['locations'])\n",
    "    slice = float(data_dict['slice'])\n",
    "    shots = int(data_dict['shots'])\n",
    "    mode = int(data_dict['mode'])\n",
    "    iterations = int(data_dict['iterations'])\n",
    "    #gray = bool(data_dict['gray'])\n",
    "    gray = ast.literal_eval(data_dict['gray'])\n",
    "    #hot_start_bool = bool(data_dict['hot_start'])\n",
    "    hot_start_bool = ast.literal_eval(data_dict['hot_start'])\n",
    "    gradient_type = data_dict['gradient_type']\n",
    "    formulation = data_dict['formulation']\n",
    "    alpha = float(data_dict['alpha'])\n",
    "    big_a = float(data_dict['big_a'])\n",
    "    c = float(data_dict['c'])\n",
    "    eta = float(data_dict['eta'])\n",
    "    gamma = float(data_dict['gamma'])\n",
    "    s = float(data_dict['s'])\n",
    "    print_frequency = int(data_dict['print_frequency'])\n",
    "\n",
    "    data_dict['runid'] = datalogger.runid\n",
    "    data_dict['subid'] = subdatalogger.subid\n",
    "    data_dict['cache_max_size'] = CACHE_MAX_SIZE\n",
    "\n",
    "    qubits = find_problem_size(locations, formulation)\n",
    "    data_filename = read_file_name(locations, DATA_SOURCES)\n",
    "    #Data sources are held locally to avoid downstream dependencies.  \n",
    "    #Read the data, and print out the filename and best distance held in the data.\n",
    "    best_dist = DATA_SOURCES[locations]['best']\n",
    "    data_dict['best_dist'] = best_dist\n",
    "    #Read and validate the distance array.  This checks the array is the correct shape, and is symmetric.\n",
    "    distance_array = np.genfromtxt(data_filename)\n",
    "    validate_distance_array(distance_array, locations)\n",
    "    #Define the VQC circuits with appropriate parameters\n",
    "    params = define_parameters(qubits, mode)\n",
    "    #define the cost function for this run\n",
    "    qc = vqc_circuit(qubits, params, mode)\n",
    "    cost_fn = cost_fn_fact(locations,distance_array, gray, method = formulation)\n",
    "\n",
    "    print(hot_start_bool)\n",
    "\n",
    "    if hot_start_bool:\n",
    "        hot_start_list = hot_start(distance_array, locations)\n",
    "        bin_hot_start_list =  hot_start_list_to_string(hot_start_list, locations, gray, formulation)\n",
    "        hot_start_dist = cost_fn(bin_hot_start_list)\n",
    "        init_rots = create_initial_rotations(qubits, mode, bin_hot_start_list, hot_start=True)\n",
    "    else:\n",
    "        init_rots = create_initial_rotations(qubits, mode)\n",
    "\n",
    "    bc = bind_weights(params, init_rots, qc)\n",
    "    bc.measure_all()\n",
    "\n",
    "    if hot_start_bool:\n",
    "        hot_start_dist, _, _ = cost_func_evaluate(cost_fn, bc, shots=shots, average_slice=slice)\n",
    "        data_dict['hot_start_dist'] = hot_start_dist\n",
    "    else:\n",
    "        data_dict['hot_start_dist'] = 'n/a'\n",
    "\n",
    "    t0 = time.time()\n",
    "    av_cost_list_all, lowest_list_all, sliced_cost_list_all = [], [], []\n",
    "    rots = copy.deepcopy(init_rots)\n",
    "    \n",
    "    index_list, sliced_list, lowest_list, _ , average_list, _ = \\\n",
    "    update_parameters_using_gradient(locations=locations, iterations=iterations, \n",
    "                                    print_frequency=print_frequency, params=params,\n",
    "                                    rots=rots,  \n",
    "                                    cost_fn=cost_fn, qc = qc, shots=shots, s=s, \n",
    "                                    eta=eta, average_slice=slice, gray=gray, \n",
    "                                    verbose=False, gradient_type=gradient_type,\n",
    "                                    alpha=alpha, gamma=gamma, c=c,\n",
    "                                    big_a=big_a,\n",
    "                                    method=formulation,\n",
    "                                    print_results=False\n",
    "            )\n",
    "    av_cost_list_all.append(average_list)\n",
    "    lowest_list_all.append(lowest_list) \n",
    "    sliced_cost_list_all.append(sliced_list)\n",
    "    best_dist_found, iteration_found = find_run_stats(lowest_list)\n",
    "    data_dict['best_dist_found'] = best_dist_found\n",
    "    data_dict['iteration_found'] = iteration_found\n",
    "    t1 = time.time()\n",
    "    elapsed = t1-t0\n",
    "    data_dict['elapsed'] = elapsed\n",
    "\n",
    "    items, hits, misses = cost_fn.report_cache_stats()\n",
    "    data_dict['cache_items'] = items\n",
    "    data_dict['cache_hits'] = hits\n",
    "    data_dict['cache_misses'] = misses\n",
    "    #data_dict['cache_items'] = len(cost_fn.cache)\n",
    "    #data_dict['cache_hits'] = cost_fn.cache_hits\n",
    "    #data_dict['cache_misses'] = cost_fn.cache_misses\n",
    "\n",
    "    cost_fn.clear_cache() #need to clear cache so statistics are not cumulative\n",
    "    \n",
    "    subdatalogger.save_dict_to_csv(data_dict) # don't write header\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
